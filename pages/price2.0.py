import streamlit as st
import pandas as pd
import re
from sqlalchemy import create_engine, text
from pathlib import Path
from collections import Counter

# â€” é¡µé¢é…ç½®ï¼šå®½å±å¸ƒå±€ã€æ ‡é¢˜ â€”
st.set_page_config(
    page_title="äº§å“æŠ¥ä»·ç³»ç»Ÿ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# â€” è‡ªå®šä¹‰ CSS â€”
st.markdown("""
<style>
/* ä¸»å®¹å™¨å¡ç‰‡ï¼Œæœ€å¤§å®½åº¦æ›´å¤§ */
.block-container {
    max-width: 1000px !important;
    margin: 2rem auto !important;
    background: #fff !important;
    padding: 2rem !important;
    border-radius: 10px !important;
    box-shadow: 0 4px 12px rgba(0,0,0,0.05) !important;
}
/* æ ‡é¢˜é¢œè‰² */
h1, h2 {
    color: #333 !important;
}
/* æŒ‰é’®ç¾åŒ– */
.stButton>button {
    background-color: #005B96 !important;
    color: #fff !important;
    border: none !important;
    border-radius: 5px !important;
    padding: 0.5em 1.5em !important;
    box-shadow: 0 2px 6px rgba(0,0,0,0.1) !important;
}
.stButton>button:hover {
    background-color: #004173 !important;
}
</style>
""", unsafe_allow_html=True)

# â€” é€šç”¨è®¾ç½® & æ•°æ®åº“è¿æ¥ â€”
DB_PATH = Path(__file__).resolve().parents[1] / "Product2.db"
engine = create_engine(f"sqlite:///{DB_PATH}", connect_args={"timeout":20}, echo=False)


#ä»æ•°æ®åº“è¯»å–äº§å“æ•°æ®ï¼Œå¹¶å¯¹ç»“æœè¿›è¡Œç¼“å­˜
@st.cache_data
def load_data():
    return pd.read_sql("SELECT * FROM Products", engine)
if "show_more" not in st.session_state:
    st.session_state.show_more = False
    
def is_token_in_text(token, text):
    # åŒ¹é…å®Œæ•´çš„è‹±å¯¸å•ä½
    # å‰é¢ä¸æ˜¯æ•°å­—æˆ–æ–œæ ï¼Œåé¢ä¸æ˜¯æ•°å­—ã€æ–œæ æˆ–è¿å­—ç¬¦
    return re.search(rf'(?<![\d/-]){re.escape(token)}(?![\d/\\-])', text) is not None
# å½’ä¸€åŒ–äº§å“æè¿°ï¼Œå°†å¸¸è§å˜ä½“ç»Ÿä¸€ä¸ºæ ‡å‡†å½¢å¼
def normalize_material(s: str) -> str:
    s = s.lower()
    s = s.replace('ï¼', '-').replace('â€”', '-').replace('â€“', '-')
    s = re.sub(r'[\s_]', '', s)
    s = s.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    s = ''.join([chr(ord(c)-65248) if 65281 <= ord(c) <= 65374 else c for c in s])
    # æè´¨å½’ä¸€åŒ–
    s = re.sub(r'pp[\s\-_â€”â€“]?[rï½’r]', 'ppr', s)  # å½’ä¸€åŒ–pp-rã€pp rã€pp_rã€ppâ€”rã€ppâ€“rã€ppï½’ä¸ºppr
    s = s.replace('pvcu', 'pvc')
    s = s.replace('pvc-u', 'pvc')
    s = s.replace('pvc u', 'pvc')
    # åªæŠŠå¸¸è§åˆ†éš”ç¬¦æ›¿æ¢æˆç©ºæ ¼ï¼Œä¿ç•™*å·
    s = re.sub(r'[\|,;ï¼Œï¼›]', ' ', s)
    s = re.sub(r'\s+', ' ', s)
    # ç»Ÿä¸€è‹±å¯¸ç¬¦å·
    s = s.replace('ï¼‚', '"').replace('"', '"').replace('"', '"')
    s = re.sub(r'\\s*\"\\s*', '"', s)  # å»é™¤è‹±å¯¸ç¬¦å·å‰åç©ºæ ¼
    s = s.replace('in', '"')           # 2in -> 2"
    s = s.replace('è‹±å¯¸', '"')
    # å¯æ ¹æ®å®é™…æƒ…å†µæ·»åŠ æ›´å¤šå˜ä½“
    return s.strip()

# æ’å…¥æ–°äº§å“çš„å‡½æ•°
def insert_product(values: dict):
    values.pop("åºå·", None)
    cols   = ", ".join(values.keys())
    params = ", ".join(f":{k}" for k in values)
    sql    = text(f"INSERT INTO Products ({cols}) VALUES ({params})")
    with engine.begin() as conn:
        conn.execute(sql, values)

def delete_products(materials: list[str]):
    if not materials:
        return
    with engine.begin() as conn:
        for m in materials:
            conn.execute(text("DELETE FROM Products WHERE Material = :m"), {"m": m})

# â€” åŒä¹‰è¯ & å•ä½æ˜ å°„å·¥å…· â€”
SYNONYM_GROUPS = [
    {"ç›´æ¥", "ç›´æ¥å¤´", "ç›´é€š"},
    {"å¤§å°å¤´", "å¼‚å¾„ç›´é€š", "å¼‚å¾„å¥—"},
    {"æ‰«é™¤å£", "æ¸…æ‰«å£"}
]

# PVCç®¡é“è‹±å¯¸-æ¯«ç±³å¯¹ç…§
mm_to_inch_pvc = {
    "16": '1/2"', "20": '3/4"', "25": '1"', "35": '1-1/4"', "40": '1-1/2"', "50": '2"',
    "65": '2-1/2"', "75": '3"', "100": '4"', "125": '5"', "150": '6"', "200": '8"',
    "250": '10"', "300": '12"'
}
inch_to_mm_pvc = {v: k for k, v in mm_to_inch_pvc.items()}

# PPRç®¡é“è‹±å¯¸-æ¯«ç±³å¯¹ç…§
mm_to_inch_ppr = {
    "20": '1/2"', "25": '3/4"', "32": '1"', "40": '1-1/4"', "50": '1-1/2"', "63": '2"',
    "75": '2-1/2"', "90": '3"', "110": '4"', "160": '6"'
}
inch_to_mm_ppr = {v: k for k, v in mm_to_inch_ppr.items()}

#æŸ¥æ‰¾æŸä¸ªè¯çš„åŒä¹‰è¯é›†åˆï¼Œç”¨äºåç»­æ£€ç´¢æ—¶è‡ªåŠ¨æ‰©å±•åŒä¹‰è¯åŒ¹é…ã€‚å¦‚æœæ²¡æœ‰åŒä¹‰è¯ï¼Œå°±åªè¿”å›è‡ªå·±ã€‚
def get_synonym_words(word):
    for group in SYNONYM_GROUPS:
        if word in group:
            return group
    return {word}

# æ‰©å±•å•ä½ç¬¦å·ï¼Œæ¯”å¦‚dn20*20ï¼Œä¼šæ‰©å±•ä¸ºdn20ã€dn20*20ã€20ã€20*20
def expand_unit_tokens(token, material=None):
    eqs = {token}
    # é€‰æ‹©å¯¹ç…§è¡¨
    if material == "pvc":
        mm_to_inch = mm_to_inch_pvc
        inch_to_mm = inch_to_mm_pvc
    elif material == "ppr":
        mm_to_inch = mm_to_inch_ppr
        inch_to_mm = inch_to_mm_ppr
    else:
        mm_to_inch = {**mm_to_inch_pvc, **mm_to_inch_ppr}
        inch_to_mm = {**inch_to_mm_pvc, **inch_to_mm_ppr}
    if token.startswith('dn'):
        num = token[2:]
        if num in mm_to_inch:
            eqs.add(mm_to_inch[num])
        eqs.add(num)
    else:
        if token in mm_to_inch:
            eqs.add(mm_to_inch[token])
            eqs.add('dn' + token)
        if token in inch_to_mm:
            eqs.add(inch_to_mm[token])
    return eqs

#å‰ä¸¤ä¸ªå‡½æ•°çš„é›†åˆ
def expand_token_with_synonyms_and_units(token, material=None):
    # å…ˆæŸ¥åŒä¹‰è¯ç»„
    synonyms = get_synonym_words(token)
    expanded = set()
    for syn in synonyms:
        expanded |= expand_unit_tokens(syn, material=material)
    return expanded

# å°†ä¸­æ–‡æè¿°åˆ‡åˆ†ä¸ºå•è¯åˆ—è¡¨ï¼Œå¹¶è‡ªåŠ¨æ‰©å±•åŒä¹‰è¯å’Œå•ä½ç¬¦å·ï¼Œ
# "PPR dn20*25 ç›´æ¥å¤´"ï¼Œ['ppr', 'dn20*25', '20*25', '20', '25', 'ç›´æ¥', 'å¤´']
def split_with_synonyms(text):
    # 0. é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–å„ç§å¯èƒ½é€ æˆè§£æé—®é¢˜çš„å­—ç¬¦
    text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    text = text.replace('ï¼‚', '"')  # å…¨è§’å¼•å·
    text = text.replace('ï¼', '-')  # å…¨è§’è¿å­—ç¬¦

    # é¢„åˆ†è¯ï¼šè§£å†³ "PPRDN20" è¿™ç±»è¿å†™é—®é¢˜
    text = re.sub(r'([A-Z])(DN)', r'\1 \2', text, flags=re.IGNORECASE)
    text = re.sub(r'(DN)(\d)', r'\1 \2', text, flags=re.IGNORECASE)

    # ç§»é™¤æ‹¬å·ï¼Œé˜²æ­¢å¹²æ‰°è‹±å¯¸è§„æ ¼è§£æ
    text = text.replace('(', ' ').replace(')', ' ')

    text = text.lower()
    text = text.replace('*', ' * ')

    tokens = []
    # å…ˆæå– dn+æ•°å­—*æ•°å­—
    pattern_dn_star = re.compile(r'dn(\d+)\*(\d+)')
    for m in pattern_dn_star.finditer(text):
        tokens.append(m.group())
        tokens.append(f"{m.group(1)}*{m.group(2)}")
        tokens.append(m.group(1))
        tokens.append(m.group(2))
    text = pattern_dn_star.sub(' ', text)
    # å†æå– dn+æ•°å­—
    pattern_dn = re.compile(r'dn(\d+)')
    for m in pattern_dn.finditer(text):
        tokens.append(m.group())
        tokens.append(m.group(1))
    text = pattern_dn.sub(' ', text)
    # å†æå– æ•°å­—*æ•°å­—
    pattern_num = re.compile(r'(\d+)\*(\d+)')
    for m in pattern_num.finditer(text):
        tokens.append(m.group())
        tokens.append(m.group(1))
        tokens.append(m.group(2))
    text = pattern_num.sub(' ', text)
    
    # ä¿®æ­£ï¼šç§»é™¤è‹±å¯¸æ­£åˆ™è¡¨è¾¾å¼ä¸­ä¸å¿…è¦çš„åæ–œæ 
    pattern_inch = re.compile(r'\d+-\d+/\d+"|\d+/\d+"|\d+"')
    for m in pattern_inch.finditer(text):
        tokens.append(m.group())
    text = pattern_inch.sub(' ', text)

    # å†æå–è¿ç»­è‹±æ–‡/æ‹¼éŸ³
    pattern_en = re.compile(r'[a-zA-Z]+')
    for m in pattern_en.finditer(text):
        tokens.append(m.group())
    text = pattern_en.sub(' ', text)
    # å†æå–å•ä¸ªæ•°å­—
    pattern_digit = re.compile(r'\d+')
    for m in pattern_digit.finditer(text):
        tokens.append(m.group())
    text = pattern_digit.sub(' ', text)
    # å‰©ä¸‹çš„æŒ‰å•å­—åˆ‡åˆ†
    tokens += [c for c in text if c.strip()]
    
    # å»æ‰ 'dnXX'ï¼Œå¦‚æœ 'XX' ä¹Ÿåœ¨ tokens é‡Œ
    filtered = []
    token_set = set(tokens)
    for t in tokens:
        if re.fullmatch(r'dn(\d+)', t):
            num = t[2:]
            if num in token_set:
                continue  # è·³è¿‡ 'dnXX'
        filtered.append(t)
    return filtered

#å‰ä¸‰å‡½æ•°æ€»å’Œï¼Œè¾“å…¥ï¼š"PPR dn20*25 ç›´æ¥å¤´"
#è¾“å‡ºï¼šmaterial_tokens: ['ppr']
#digit_tokens: ['2', '0', '2', '5']
#chinese_tokens: ['ppr', 'dn20*25', '20*25', '20', '25', 'ç›´æ¥', 'å¤´']
def classify_tokens(keyword):
    norm_kw = normalize_material(keyword)
    # æè´¨
    material_tokens = re.findall(r'pvc|ppr|pe|pp|hdpe|pb|pert', norm_kw)
    # æ•°å­—
    digit_tokens = re.findall(r'\d', norm_kw)
    # ä¸­æ–‡åŒä¹‰è¯æ•´ä½“åˆ‡åˆ†
    chinese_tokens = split_with_synonyms(keyword)
    return material_tokens, digit_tokens, chinese_tokens


def search_with_keywords(df, keyword, field, strict=True, return_score=False):
    material_tokens, _, chinese_tokens = classify_tokens(keyword.strip())
    
    query_size_tokens = {t for t in chinese_tokens if re.search(r'\d', t)}
    query_text_tokens = {t for t in chinese_tokens if not re.search(r'\d', t)}

    # 1. ä¸ºæ¯ä¸ªæŸ¥è¯¢è§„æ ¼ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ç­‰ä»·å†™æ³•çš„é›†åˆ
    query_spec_equivalents = {}
    query_material = material_tokens[0] if material_tokens else None
    for token in query_size_tokens:
        query_spec_equivalents[token] = expand_token_with_synonyms_and_units(token, material=query_material)
    
    results = []
    for row in df.itertuples(index=False):
        raw_text = str(getattr(row, field, ""))
        normalized_text = normalize_material(raw_text)

        if not all(m in normalized_text for m in material_tokens):
            continue

        product_all_tokens = split_with_synonyms(raw_text)
        text_specs = {t for t in product_all_tokens if re.search(r'\d', t)}
        
        if len(query_size_tokens) > len(text_specs):
            continue
 
        if query_size_tokens:
            unmatched_text_specs = text_specs.copy()
            all_query_specs_matched = True
            for q_spec, q_equivalents in query_spec_equivalents.items():
                match_found = False
                for t_spec in list(unmatched_text_specs):
                    if t_spec in q_equivalents:
                        match_found = True
                        unmatched_text_specs.remove(t_spec)
                        break
                if not match_found:
                    all_query_specs_matched = False
                    break
            
            if not all_query_specs_matched:
                continue

        material_keywords_in_query = {'pvc', 'ppr'}.intersection(query_text_tokens)
        if material_keywords_in_query:
            if not any(mat in normalized_text.lower() for mat in material_keywords_in_query):
                continue

        hit_count = len(query_size_tokens)
        
        if strict:
            if not all(t in normalized_text.lower() for t in query_text_tokens):
                continue
            hit_count += len(query_text_tokens)
        else:
            product_text_lower = normalized_text.lower()
            for token in query_text_tokens:
                if token in product_text_lower:
                    hit_count += 1
            if query_text_tokens and hit_count == len(query_size_tokens):
                continue
        
        if return_score:
            total_tokens = len(query_size_tokens) + len(query_text_tokens)
            score = hit_count / total_tokens if total_tokens > 0 else 1
            results.append((row, score))
        else:
            results.append(row)
            
    return results

# â€” Session State åˆå§‹åŒ– â€”
for k, default in [("cart",[]),("last_out",pd.DataFrame()),("to_cart",[]),("to_remove",[])]:
    if k not in st.session_state:
        st.session_state[k] = default
#æŠŠç”¨æˆ·é€‰ä¸­çš„æŸ¥è¯¢ç»“æœæ¡ç›®åŠ å…¥è´­ç‰©è½¦ï¼Œå¹¶æ¸…ç©ºæœ¬æ¬¡é€‰æ‹©ï¼Œæ”¯æŒå¤šé€‰æ‰¹é‡æ·»åŠ 
def add_to_cart():
    for i in st.session_state.to_cart:
        st.session_state.cart.append(st.session_state.last_out.loc[i].to_dict())
    # æ¸…ç©ºé€‰æ‹©ï¼ˆæ¨èç”¨ pop æˆ– delï¼‰
    if "to_cart" in st.session_state:
        del st.session_state["to_cart"]

#åˆ é™¤è´­ç‰©è½¦ä¸­çš„æ¡ç›®ï¼Œæ”¯æŒå¤šé€‰æ‰¹é‡åˆ é™¤
def remove_from_cart():
    idxs = set(st.session_state.to_remove)
    st.session_state.cart = [it for j,it in enumerate(st.session_state.cart) if j not in idxs]
    if "to_remove" in st.session_state:
        del st.session_state["to_remove"]

# â€” ä¾§è¾¹æ å¯¼èˆª â€”
st.sidebar.header("å¯¼  èˆª")
page = st.sidebar.radio("æ“ä½œ", ["æŸ¥è¯¢äº§å“","æ·»åŠ äº§å“","åˆ é™¤äº§å“"])
st.sidebar.markdown("---")
st.sidebar.caption("Powered by Streamlit")


# é¡µé¢åˆ‡æ¢å’Œä¸»é€»è¾‘
if page == "æŸ¥è¯¢äº§å“":
    st.header("äº§å“æŸ¥è¯¢ç³»ç»Ÿ")
    df = load_data()
#è¾“å…¥æ¡†å¸ƒå±€
    c1, c3 = st.columns([6,1])
    with c1:
        keyword = st.text_input(
            "å…³é”®è¯ï¼ˆåç§°ã€è§„æ ¼ã€æè´¨å¯ä¸€èµ·è¾“å…¥ï¼‰",
            key="keyword"
        )
    with c3:
        qty = st.number_input(
            "æ•°é‡", min_value=1, key="qty"
        )
    mat_kw = st.text_input(
        "ç‰©æ–™å·æœç´¢", key="mat_kw"
    )
    price_type = st.selectbox(
        "ä»·æ ¼å­—æ®µ", ["å‡ºå‚ä»·_å«ç¨","å‡ºå‚ä»·_ä¸å«ç¨"],
        key="price_type"
    )
    fuzzy_mode = st.checkbox(
        "æœªæŸ¥åˆ°ç»“æœæ—¶å¯ç”¨æ¨¡ç³ŠæŸ¥æ‰¾ï¼ˆå¹¶æ˜¾ç¤ºåŒ¹é…åº¦ï¼‰",
        key="fuzzy_mode"
    )

    #æŸ¥è¯¢æŒ‰é’®
    if st.button("æŸ¥è¯¢"):
        out_df = pd.DataFrame()
        qty = st.session_state.qty if "qty" in st.session_state else 1
        show_cols = ["Material", "Describrition", "æ•°é‡", "å‡ºå‚ä»·_å«ç¨", "å‡ºå‚ä»·_ä¸å«ç¨"]

        # ä¼˜å…ˆç‰©æ–™å·ç²¾ç¡®æŸ¥æ‰¾
        mat_kw = st.session_state.get("mat_kw", "").strip()
        if mat_kw:
            filtered = df[df["Material"].astype(str).str.contains(mat_kw)]
            if not filtered.empty:
                out_df = pd.DataFrame(filtered.copy())  # å¼ºåˆ¶DataFrame
                out_df["æ•°é‡"] = qty
                out_df = out_df[[col for col in show_cols if col in out_df.columns]]
                st.session_state.last_out = out_df
            else:
                st.session_state.last_out = pd.DataFrame()
                st.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ç¬¦åˆæ¡ä»¶çš„äº§å“")
        else:
            # åŸæœ‰å…³é”®è¯æŸ¥æ‰¾é€»è¾‘
            results = search_with_keywords(df, st.session_state.keyword, "Describrition", strict=True)
            #æ¨¡ç³ŠæŸ¥è¯¢
            if not results and st.session_state.fuzzy_mode:
                fuzzy_results = search_with_keywords(df, st.session_state.keyword, "Describrition", strict=False, return_score=True)
                if fuzzy_results:
                    out_df = pd.DataFrame([r[0] for r in fuzzy_results])
                    out_df["åŒ¹é…åº¦"] = [round(r[1], 2) for r in fuzzy_results]
                    out_df = out_df.sort_values("åŒ¹é…åº¦", ascending=False)
                    out_df["æ•°é‡"] = qty
                    show_cols_fuzzy = show_cols + ["åŒ¹é…åº¦"]
                    out_df = out_df[[col for col in show_cols_fuzzy if col in out_df.columns]]

                    # -- æ–°é€»è¾‘ï¼šå±•ç¤ºåŒ¹é…åº¦æ’åå‰ä¸‰çš„ç»“æœ --
                    # è·å–åŒ¹é…åº¦çš„å”¯ä¸€å€¼å¹¶é™åºæ’åˆ—
                    unique_scores = sorted(out_df["åŒ¹é…åº¦"].unique(), reverse=True)
                    # è·å–å‰ä¸‰é«˜çš„åˆ†æ•°
                    top_3_scores = unique_scores[:3]
                    # ç­›é€‰å‡ºåŒ¹é…åº¦åœ¨å‰ä¸‰é«˜çš„æ‰€æœ‰è¡Œ
                    top_df = out_df[out_df["åŒ¹é…åº¦"].isin(top_3_scores)]
                    st.session_state.last_out = top_df
                else:
                    st.session_state.last_out = pd.DataFrame()
                    st.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ç¬¦åˆæ¡ä»¶çš„äº§å“")
            #ç²¾å‡†æŸ¥è¯¢
            elif results:
                out_df = pd.DataFrame(results)
                out_df["æ•°é‡"] = qty
                out_df = out_df[[col for col in show_cols if col in out_df.columns]]
                st.session_state.last_out = out_df
            else:
                st.session_state.last_out = pd.DataFrame()
                st.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ç¬¦åˆæ¡ä»¶çš„äº§å“")

    # æŸ¥è¯¢ç»“æœå±•ç¤ºå’Œè´­ç‰©è½¦æ“ä½œï¼ˆæ— è®ºæ˜¯å¦åˆšç‚¹äº†æŸ¥è¯¢æŒ‰é’®ï¼Œåªè¦æœ‰ç»“æœéƒ½æ˜¾ç¤ºï¼‰
    out_df = st.session_state.get("last_out", pd.DataFrame())
    if not out_df.empty and isinstance(out_df, pd.DataFrame):
        st.dataframe(out_df, use_container_width=True)
        def format_row(i):
            try:
                row = out_df.loc[i]
                if "äº§å“æè¿°" in out_df.columns:
                    return row["äº§å“æè¿°"]
                elif "Describrition" in out_df.columns:
                    return row["Describrition"]
                elif "Material" in out_df.columns:
                    return str(row["Material"])
                else:
                    return str(i)
            except Exception:
                return str(i)
        to_cart = st.multiselect(
            "é€‰æ‹©è¦åŠ å…¥è´­ç‰©è½¦çš„è¡Œ",
            options=list(out_df.index),
            format_func=format_row,
            key="to_cart"
        )
        if st.button("æ·»åŠ åˆ°è´­ç‰©è½¦", key="add_cart"):
            for i in to_cart:
                st.session_state.cart.append(out_df.loc[i].to_dict())
            if "to_cart" in st.session_state:
                del st.session_state["to_cart"]
            st.success("âœ… å·²åŠ å…¥è´­ç‰©è½¦")

    # è´­ç‰©è½¦åªåœ¨æœ‰å†…å®¹æ—¶æ˜¾ç¤º
    if st.session_state.cart:
        cart_df = pd.DataFrame(st.session_state.cart)
        st.dataframe(cart_df, use_container_width=True)
        to_remove = st.multiselect(
            "é€‰æ‹©è¦åˆ é™¤çš„è´­ç‰©è½¦æ¡ç›®",
            options=list(cart_df.index),
            format_func=lambda i: cart_df.loc[i, "äº§å“æè¿°"] if "äº§å“æè¿°" in cart_df.columns else cart_df.loc[i, "Describrition"],
            key="to_remove"
        )
        if st.button("åˆ é™¤æ‰€é€‰", key="del_cart_bottom"):
            idxs = set(to_remove)
            st.session_state.cart = [it for j, it in enumerate(st.session_state.cart) if j not in idxs]
            if "to_remove" in st.session_state:
                del st.session_state["to_remove"]
            st.rerun()

elif page == "æ·»åŠ äº§å“":
    st.header(" æ·»åŠ æ–°äº§å“åˆ°æ•°æ®åº“")
    df0 = load_data()
    cols = df0.columns.tolist()

    with st.form("add_form"):
        new_vals = {}
        for col in cols:
            if col == "åºå·":
                continue
            label = col + ("ï¼ˆå¿…å¡«ï¼‰" if col in ["Describrition","å‡ºå‚ä»·_å«ç¨","å‡ºå‚ä»·_ä¸å«ç¨"] else "")
            dtype = df0[col].dtype
            if col in ["å‡ºå‚ä»·_å«ç¨","å‡ºå‚ä»·_ä¸å«ç¨"]:
                new_vals[col] = st.text_input(label, key=f"add_{col}")
            elif pd.api.types.is_integer_dtype(dtype):
                new_vals[col] = st.number_input(label, step=1, format="%d", key=f"add_{col}")
            elif pd.api.types.is_float_dtype(dtype):
                new_vals[col] = st.number_input(label, format="%.2f", key=f"add_{col}")
            else:
                new_vals[col] = st.text_input(label, key=f"add_{col}")

        submitted = st.form_submit_button("æäº¤æ–°å¢")

    if submitted:
        missing = [
            f for f in ["Describrition","å‡ºå‚ä»·_å«ç¨","å‡ºå‚ä»·_ä¸å«ç¨"]
            if not new_vals.get(f) or (isinstance(new_vals[f], str) and not new_vals[f].strip())
        ]
        if missing:
            st.error(f"âš ï¸ ä»¥ä¸‹å­—æ®µä¸ºå¿…å¡«ï¼š{', '.join(missing)}")
        else:
            insert_product(new_vals)
            load_data.clear()
            st.success("âœ… äº§å“å·²æ·»åŠ åˆ°æ•°æ®åº“ï¼")

else:
    st.header("ğŸ—‘ï¸ åˆ é™¤äº§å“")
    df = load_data()
    if df.empty:
        st.info("å½“å‰æ— äº§å“å¯åˆ é™¤ã€‚")
    else:
        materials = st.multiselect(
            "è¯·é€‰æ‹©è¦åˆ é™¤çš„äº§å“ (Material)",
            options=df["Material"].tolist(),
            format_func=lambda m: str(m)
        )
        if st.button("åˆ é™¤é€‰ä¸­äº§å“"):
            delete_products(materials)
            load_data.clear()
            st.success("âœ… åˆ é™¤æˆåŠŸï¼")

def is_token_in_text(token, text):
    # åŒ¹é…å®Œæ•´çš„è‹±å¯¸å•ä½
    # å‰é¢ä¸æ˜¯æ•°å­—æˆ–æ–œæ ï¼Œåé¢ä¸æ˜¯æ•°å­—ã€æ–œæ æˆ–è¿å­—ç¬¦
    return re.search(rf'(?<![\d/-]){re.escape(token)}(?![\d/\\-])', text) is not None



