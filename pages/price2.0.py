import streamlit as st
import pandas as pd
import re
from sqlalchemy import create_engine, text
from sqlalchemy.pool import StaticPool
from pathlib import Path
from collections import Counter
from openai import OpenAI

# --- å®‰å…¨åœ°ä» Streamlit Secrets è·å– API KEY ---
OPENAI_API_KEY = st.secrets.get("OPENAI_API_KEY")

# --- AI é€‰æ‹©åŠŸèƒ½ (GPT-4o-mini) ---
def ai_select_best_with_gpt(keyword: str, df: pd.DataFrame):
    """
    Uses GPT-4o-mini to select the best match from a DataFrame of candidates.
    """
    if not OPENAI_API_KEY:
        return None, "é”™è¯¯ï¼šè¯·åœ¨ Streamlit Cloud çš„ Secrets ä¸­è®¾ç½®æ‚¨çš„ OpenAI API Keyã€‚"

    client = OpenAI(api_key=OPENAI_API_KEY)

    # Create a string representation of the choices for the prompt
    choices_str = ""
    # Use a fresh, reset index for this operation to guarantee alignment
    df_reset = df.reset_index(drop=True)
    for i, row in df_reset.iterrows():
        choices_str += f"ç´¢å¼• {i}: {row['Describrition']}\n" # ä½¿ç”¨çœŸå®çš„æ¢è¡Œç¬¦

    prompt_lines = [
        "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®¡é“å»ºæäº§å“é‡‡è´­ä¸“å®¶ã€‚ä½ çš„ä»»åŠ¡æ˜¯ä»ä¸€ä¸ªäº§å“åˆ—è¡¨ä¸­ï¼Œæ ¹æ®ç”¨æˆ·çš„æœç´¢è¯·æ±‚ï¼Œé€‰å‡ºæœ€åŒ¹é…çš„ä¸€é¡¹ã€‚",
        f"ç”¨æˆ·çš„æœç´¢è¯·æ±‚æ˜¯: \"{keyword}\"",
        f"ä»¥ä¸‹æ˜¯ç³»ç»Ÿæ¨¡ç³ŠåŒ¹é…å‡ºçš„æœ€ç›¸å…³çš„{len(df_reset)}ä¸ªå€™é€‰äº§å“:",
        "---",
        choices_str,
        "---",
        "è¯·ä»”ç»†åˆ†æç”¨æˆ·çš„è¯·æ±‚å’Œæ¯ä¸ªå€™é€‰äº§å“çš„æè¿°ï¼Œé€‰å‡ºæœ€ç¬¦åˆç”¨æˆ·æ„å›¾çš„**å”¯ä¸€ä¸€ä¸ª**äº§å“ã€‚",
        "ä½ çš„å›ç­”å¿…é¡»ä¸¥æ ¼éµå¾ªä»¥ä¸‹æ ¼å¼ï¼Œåªè¿”å›ä½ é€‰æ‹©çš„äº§å“çš„**ç´¢å¼•æ•°å­—**ï¼Œä¸è¦æ·»åŠ ä»»ä½•å…¶ä»–å†…å®¹ã€‚",
        "ä¾‹å¦‚:",
        "2"
    ]
    prompt = "\n".join(prompt_lines)

    try:
        response = client.chat.completions.create(
            model="gpt-4.1-nano",
            messages=[
                {"role": "system", "content": "ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„ç®¡é“å»ºæäº§å“é‡‡è´­ä¸“å®¶ã€‚"},
                {"role": "user", "content": prompt}
            ],
            temperature=0,
            max_tokens=10, # Only need a number
            timeout=20,
        )
        
        content = response.choices[0].message.content.strip()
        
        if not content.isdigit():
             raise ValueError(f"AIæœªèƒ½è¿”å›æœ‰æ•ˆçš„ç´¢å¼•æ•°å­—ã€‚åŸå§‹å›å¤: '{content}'")

        selected_index = int(content)

        if selected_index < 0 or selected_index >= len(df_reset):
             raise ValueError(f"AIè¿”å›äº†è¶Šç•Œçš„ç´¢å¼•: {selected_index}ã€‚")

        # Return the single selected row (using the correct index)
        best_row_df = df_reset.iloc[[selected_index]]
        return best_row_df, "Success"

    except Exception as e:
        error_message = str(e)
        if "Incorrect API key" in error_message:
            return None, "AIè°ƒç”¨å¤±è´¥ï¼šAPI Keyä¸æ­£ç¡®æˆ–å·²å¤±æ•ˆã€‚è¯·æ£€æŸ¥ Streamlit Cloud ä¸­çš„é…ç½®ã€‚"
        return None, f"AIè°ƒç”¨å¤±è´¥ï¼š{error_message}"
# --- ç»“æŸ AI åŠŸèƒ½ ---

# â€” é¡µé¢é…ç½®ï¼šå®½å±å¸ƒå±€ã€æ ‡é¢˜ â€”
st.set_page_config(
    page_title="äº§å“æŠ¥ä»·ç³»ç»Ÿ",
    layout="wide",
    initial_sidebar_state="expanded"
)

# â€” è‡ªå®šä¹‰ CSS â€”
st.markdown("""
<style>
/* ä¸»å®¹å™¨å¡ç‰‡ï¼Œæœ€å¤§å®½åº¦æ›´å¤§ */
.block-container {
    max-width: 1000px !important;
    margin: 2rem auto !important;
    background: #fff !important;
    padding: 2rem !important;
    border-radius: 10px !important;
    box-shadow: 0 4px 12px rgba(0,0,0,0.05) !important;
}
/* æ ‡é¢˜é¢œè‰² */
h1, h2 {
    color: #333 !important;
}
/* æŒ‰é’®ç¾åŒ– */
.stButton>button {
    background-color: #005B96 !important;
    color: #fff !important;
    border: none !important;
    border-radius: 5px !important;
    padding: 0.5em 1.5em !important;
    box-shadow: 0 2px 6px rgba(0,0,0,0.1) !important;
}
.stButton>button:hover {
    background-color: #004173 !important;
}
</style>
""", unsafe_allow_html=True)

# â€” é€šç”¨è®¾ç½® & æ•°æ®åº“è¿æ¥ â€”
@st.cache_resource
def get_db_engine():
    """
    Creates a cached database engine for the Streamlit app.
    Using @st.cache_resource ensures that the connection is established only once
    per session. The StaticPool is crucial for SQLite to prevent "database is locked"
    errors in Streamlit's multi-threaded environment by ensuring all operations
    use the same underlying connection.
    """
    DB_PATH = Path(__file__).resolve().parents[1] / "Product2.db"
    engine = create_engine(
        f"sqlite:///{DB_PATH}",
        connect_args={"check_same_thread": False},
        poolclass=StaticPool,
        echo=False
    )
    return engine

engine = get_db_engine()


#ä»æ•°æ®åº“è¯»å–äº§å“æ•°æ®ï¼Œå¹¶å¯¹ç»“æœè¿›è¡Œç¼“å­˜
@st.cache_data
def load_data():
    return pd.read_sql("SELECT * FROM Products", engine)
if "show_more" not in st.session_state:
    st.session_state.show_more = False
    
def is_token_in_text(token, text):
    # åŒ¹é…å®Œæ•´çš„è‹±å¯¸å•ä½
    # å‰é¢ä¸æ˜¯æ•°å­—æˆ–æ–œæ ï¼Œåé¢ä¸æ˜¯æ•°å­—ã€æ–œæ æˆ–è¿å­—ç¬¦
    return re.search(rf'(?<![\d/-]){re.escape(token)}(?![\d/\\-])', text) is not None
# å½’ä¸€åŒ–äº§å“æè¿°ï¼Œå°†å¸¸è§å˜ä½“ç»Ÿä¸€ä¸ºæ ‡å‡†å½¢å¼
def normalize_material(s: str) -> str:
    s = s.lower()
    s = s.replace('ï¼', '-').replace('â€”', '-').replace('â€“', '-')
    s = re.sub(r'[_\t]', ' ', s)
    s = s.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    s = s.replace('x', '*') # ç»Ÿä¸€å°ºå¯¸åˆ†éš”ç¬¦
    s = ''.join([chr(ord(c)-65248) if 65281 <= ord(c) <= 65374 else c for c in s])
    # æè´¨å½’ä¸€åŒ–
    s = re.sub(r'pp[\s\-_â€”â€“]?[rï½’r]', 'ppr', s)  # å½’ä¸€åŒ–pp-rã€pp rã€pp_rã€ppâ€”rã€ppâ€“rã€ppï½’ä¸ºppr
    s = s.replace('pvcu', 'pvc')
    s = s.replace('pvc-u', 'pvc')
    s = s.replace('pvc u', 'pvc')
    # åªæŠŠå¸¸è§åˆ†éš”ç¬¦æ›¿æ¢æˆç©ºæ ¼ï¼Œä¿ç•™*å·
    s = re.sub(r'[\|,;ï¼Œï¼›ã€]', ' ', s)
    s = re.sub(r'\s+', ' ', s)
    # ç»Ÿä¸€è‹±å¯¸ç¬¦å·
    s = s.replace('ï¼‚', '"').replace('"', '"').replace('"', '"')
    s = re.sub(r'\\s*\"\\s*', '"', s)  # å»é™¤è‹±å¯¸ç¬¦å·å‰åç©ºæ ¼
    s = s.replace('in', '"')           # 2in -> 2"
    s = s.replace('è‹±å¯¸', '"')
    s = s.replace('å¯¸', '"')
    # å¯æ ¹æ®å®é™…æƒ…å†µæ·»åŠ æ›´å¤šå˜ä½“
    return s.strip()

# æ’å…¥æ–°äº§å“çš„å‡½æ•°
def insert_product(values: dict):
    values.pop("åºå·", None)
    cols   = ", ".join(values.keys())
    params = ", ".join(f":{k}" for k in values)
    sql    = text(f"INSERT INTO Products ({cols}) VALUES ({params})")
    with engine.begin() as conn:
        conn.execute(sql, values)

def delete_products(materials: list[str]):
    if not materials:
        return
    with engine.begin() as conn:
        for m in materials:
            conn.execute(text("DELETE FROM Products WHERE Material = :m"), {"m": m})

# â€” åŒä¹‰è¯ & å•ä½æ˜ å°„å·¥å…· â€”
SYNONYM_GROUPS = [
    {"ç›´æ¥", "ç›´æ¥å¤´", "ç›´é€š"},
    {"å¤§å°å¤´", "å¼‚å¾„ç›´é€š", "å¼‚å¾„å¥—"},
    {"æ‰«é™¤å£", "æ¸…æ‰«å£", "æ£€æŸ¥å£"},
    {"å†…ä¸", "å†…èºçº¹"},
    {"åŒè”", "åŒè”åº§"}
]

# PVCç®¡é“è‹±å¯¸-æ¯«ç±³å¯¹ç…§
mm_to_inch_pvc = {
    "16": '1/2"', "20": '3/4"', "25": '1"', "35": '1-1/4"', "40": '1-1/2"', "50": '2"',
    "65": '2-1/2"', "75": '3"', "100": '4"', "125": '5"', "150": '6"', "200": '8"',
    "250": '10"', "300": '12"'
}
inch_to_mm_pvc = {v: k for k, v in mm_to_inch_pvc.items()}

# PPRç®¡é“è‹±å¯¸-æ¯«ç±³å¯¹ç…§
mm_to_inch_ppr = {
    "20": '1/2"', "25": '3/4"', "32": '1"', "40": '1-1/4"', "50": '1-1/2"', "63": '2"',
    "75": '2-1/2"', "90": '3"', "110": '4"', "160": '6"'
}
inch_to_mm_ppr = {v: k for k, v in mm_to_inch_ppr.items()}

#æŸ¥æ‰¾æŸä¸ªè¯çš„åŒä¹‰è¯é›†åˆï¼Œç”¨äºåç»­æ£€ç´¢æ—¶è‡ªåŠ¨æ‰©å±•åŒä¹‰è¯åŒ¹é…ã€‚å¦‚æœæ²¡æœ‰åŒä¹‰è¯ï¼Œå°±åªè¿”å›è‡ªå·±ã€‚
def get_synonym_words(word):
    for group in SYNONYM_GROUPS:
        if word in group:
            return group
    return {word}

# æ‰©å±•å•ä½ç¬¦å·ï¼Œæ¯”å¦‚dn20*20ï¼Œä¼šæ‰©å±•ä¸ºdn20ã€dn20*20ã€20ã€20*20
def expand_unit_tokens(token, material=None):
    eqs = {token}
    # é€‰æ‹©å¯¹ç…§è¡¨
    if material == "pvc":
        mm_to_inch = mm_to_inch_pvc
        inch_to_mm = inch_to_mm_pvc
    elif material == "ppr":
        mm_to_inch = mm_to_inch_ppr
        inch_to_mm = inch_to_mm_ppr
    else:
        mm_to_inch = {**mm_to_inch_pvc, **mm_to_inch_ppr}
        inch_to_mm = {**inch_to_mm_pvc, **inch_to_mm_ppr}
    
    # Case 0: Handle composite specs like "20*1/2""
    m = re.fullmatch(r'(?:dn)?(\d+)\*(.+)', token)
    if m:
        part1_mm = m.group(1)
        part2_inch_str = m.group(2)
        if part1_mm in mm_to_inch:
            eqs.add(f"dn{part1_mm}*{part2_inch_str}")
            eqs.add(f"{part1_mm}*{part2_inch_str}")
        return eqs

    # Case 1: 'dn' value, e.g., 'dn25'
    if token.startswith('dn'):
        num = token[2:]
        if num in mm_to_inch:
            eqs.add(mm_to_inch[num]) # '3/4"'
        eqs.add(num) # '25'
        return eqs

    # Case 2: An inch value, quoted or not, e.g., '3/4"' or '3/4'
    inch_lookup_token = token
    # Add quote if it's a fraction like "1/2", "1-1/2"
    if re.fullmatch(r'\d+-\d+/\d+|\d+/\d+', token):
        inch_lookup_token = token + '"' # '3/4' -> '3/4"'
    
    if inch_lookup_token in inch_to_mm:
        mm_val = inch_to_mm[inch_lookup_token] # '25'
        eqs.add(mm_val)
        eqs.add('dn' + mm_val) # 'dn25'
        eqs.add(inch_lookup_token) # '3/4"'
        return eqs

    # Case 3: A plain number, could be mm, e.g., '25'
    if token.isdigit() and token in mm_to_inch:
        eqs.add('dn' + token) # 'dn25'
        eqs.add(mm_to_inch[token]) # '3/4"'
        return eqs

    return eqs


#å‰ä¸¤ä¸ªå‡½æ•°çš„é›†åˆ
def expand_token_with_synonyms_and_units(token, material=None):
    # å…ˆæŸ¥åŒä¹‰è¯ç»„
    synonyms = get_synonym_words(token)
    expanded = set()
    for syn in synonyms:
        expanded |= expand_unit_tokens(syn, material=material)
    return expanded

# å°†ä¸­æ–‡æè¿°åˆ‡åˆ†ä¸ºå•è¯åˆ—è¡¨ï¼Œå¹¶è‡ªåŠ¨æ‰©å±•åŒä¹‰è¯å’Œå•ä½ç¬¦å·ï¼Œ
# "PPR dn20*25 ç›´æ¥å¤´"ï¼Œ['ppr', 'dn20*25', '20*25', '20', '25', 'ç›´æ¥', 'å¤´']
def split_with_synonyms(text):
    # 0. é¢„å¤„ç†ï¼šæ ‡å‡†åŒ–å„ç§å¯èƒ½é€ æˆè§£æé—®é¢˜çš„å­—ç¬¦
    text = text.replace('ï¼ˆ', '(').replace('ï¼‰', ')')
    text = text.replace('ï¼‚', '"')  # å…¨è§’å¼•å·
    text = text.replace('ï¼', '-')  # å…¨è§’è¿å­—ç¬¦

    # é¢„åˆ†è¯ï¼šè§£å†³ "PPRDN20" è¿™ç±»è¿å†™é—®é¢˜
    text = re.sub(r'([A-Z])(DN)', r'\1 \2', text, flags=re.IGNORECASE)
    text = re.sub(r'(DN)(\d)', r'\1 \2', text, flags=re.IGNORECASE)

    # ç§»é™¤æ‹¬å·ï¼Œé˜²æ­¢å¹²æ‰°è‹±å¯¸è§„æ ¼è§£æ
    text = text.replace('(', ' ').replace(')', ' ')

    text = text.lower()
    text = text.replace('*', ' * ')
    # ç»Ÿä¸€é¡¿å·ã€é€—å·ç­‰
    text = text.replace('ã€', ' ').replace('ï¼Œ', ' ').replace('ï¼›', ' ')

    # æ–°å¢ï¼šç»Ÿä¸€æ•°å­—å’Œè‹±å¯¸å•ä½çš„ç»„åˆ
    text = re.sub(r'(\d+(?:\.\d+)?)\s*(in|å¯¸|è‹±å¯¸)', r'\1"', text)

    tokens = []

    # NEW: ä¼˜å…ˆæå–é”®å€¼å¯¹, å¦‚ "pn=1.0", "pn:10"
    # è¿™æ ·å¯ä»¥æ­£ç¡®åœ°å°†é”®å’Œå€¼åˆ†å¼€ï¼Œå¹¶ä¸”èƒ½å¤„ç†æµ®ç‚¹æ•°
    pattern_kv = re.compile(r'([a-zA-Z]+)\s*[:=]\s*(\d+(?:\.\d+)?)')
    for m in pattern_kv.finditer(text):
        tokens.append(m.group(1))  # key, e.g., "pn"
        tokens.append(m.group(2))  # value, e.g., "1.0"
    text = pattern_kv.sub(' ', text)

    # NEW: Handle mixed mm*inch specs first
    # e.g., dn20*1/2"
    pattern_dn_mixed = re.compile(r'dn(\d+)\*(\d+/\d+"|\d+")')
    for m in pattern_dn_mixed.finditer(text):
        tokens.append(m.group(0)) # dn20*1/2"
        tokens.append(m.group(1)) # 20
        tokens.append(m.group(2)) # 1/2"
    text = pattern_dn_mixed.sub(' ', text)

    # e.g., 20*1/2"
    pattern_mixed = re.compile(r'(\d+)\*(\d+/\d+"|\d+")')
    for m in pattern_mixed.finditer(text):
        tokens.append(m.group(0)) # 20*1/2"
        tokens.append(m.group(1)) # 20
        tokens.append(m.group(2)) # 1/2"
    text = pattern_mixed.sub(' ', text)
    
    # æ–°å¢ï¼šå¤„ç†è§’åº¦è§„æ ¼ï¼Œå¦‚ 90Â°
    pattern_angle = re.compile(r'\d+Â°')
    for m in pattern_angle.finditer(text):
        tokens.append(m.group(0))
    text = pattern_angle.sub(' ', text)

    # å…ˆæå– dn+æ•°å­—*æ•°å­—
    pattern_dn_star = re.compile(r'dn(\d+)\*(\d+)')
    for m in pattern_dn_star.finditer(text):
        tokens.append(m.group())
        tokens.append(f"{m.group(1)}*{m.group(2)}")
        tokens.append(m.group(1))
        tokens.append(m.group(2))
    text = pattern_dn_star.sub(' ', text)
    # å†æå– dn+æ•°å­—
    pattern_dn = re.compile(r'dn(\d+)')
    for m in pattern_dn.finditer(text):
        tokens.append(m.group())
        tokens.append(m.group(1))
    text = pattern_dn.sub(' ', text)
    # å†æå– æ•°å­—*æ•°å­—
    pattern_num = re.compile(r'(\d+)\*(\d+)')
    for m in pattern_num.finditer(text):
        tokens.append(m.group())
        tokens.append(m.group(1))
        tokens.append(m.group(2))
    text = pattern_num.sub(' ', text)
    
    # ä¿®æ­£ï¼šæ›´æ–°è‹±å¯¸æ­£åˆ™è¡¨è¾¾å¼ä»¥æ”¯æŒå°æ•°ç‚¹
    pattern_inch = re.compile(r'\d+-\d+/\d+"|\d+/\d+"|(?:\d+\.\d+|\d+)"')
    for m in pattern_inch.finditer(text):
        tokens.append(m.group())
    text = pattern_inch.sub(' ', text)

    # æ–°å¢: æå–ä¸å¸¦å¼•å·çš„åˆ†æ•° (e.g. 3/4, 1-1/2)
    pattern_fraction = re.compile(r'\d+-\d+/\d+|\d+/\d+')
    for m in pattern_fraction.finditer(text):
        tokens.append(m.group())
    text = pattern_fraction.sub(' ', text)

    # å†æå–è¿ç»­è‹±æ–‡/æ‹¼éŸ³
    pattern_en = re.compile(r'[a-zA-Z]+')
    for m in pattern_en.finditer(text):
        tokens.append(m.group())
    text = pattern_en.sub(' ', text)
    # å†æå–å•ä¸ªæ•°å­— (åŒ…æ‹¬å°æ•°)
    pattern_digit = re.compile(r'\d+(?:\.\d+)?')
    for m in pattern_digit.finditer(text):
        tokens.append(m.group())
    text = pattern_digit.sub(' ', text)
    # å‰©ä¸‹çš„æŒ‰å•å­—åˆ‡åˆ†
    tokens += [c for c in text if c.strip()]
    
    # å»æ‰ 'dnXX'ï¼Œå¦‚æœ 'XX' ä¹Ÿåœ¨ tokens é‡Œ
    filtered = []
    token_set = set(tokens)
    for t in tokens:
        if re.fullmatch(r'dn(\d+)', t):
            num = t[2:]
            if num in token_set:
                continue  # è·³è¿‡ 'dnXX'
        filtered.append(t)
    return filtered

#å‰ä¸‰å‡½æ•°æ€»å’Œï¼Œè¾“å…¥ï¼š"PPR dn20*25 ç›´æ¥å¤´"
#è¾“å‡ºï¼šmaterial_tokens: ['ppr']
#digit_tokens: ['2', '0', '2', '5']
#chinese_tokens: ['ppr', 'dn20*25', '20*25', '20', '25', 'ç›´æ¥', 'å¤´']
def classify_tokens(keyword):
    norm_kw = normalize_material(keyword)
    # æè´¨
    material_tokens = re.findall(r'pvc|ppr|pe|pp|hdpe|pb|pert', norm_kw)
    # æ•°å­— (ä¿®æ­£ï¼šåŒ¹é…åŒ…æ‹¬å°æ•°åœ¨å†…çš„å®Œæ•´æ•°å­—)
    digit_tokens = re.findall(r'\d+(?:\.\d+)?', norm_kw)
    # ä¸­æ–‡åŒä¹‰è¯æ•´ä½“åˆ‡åˆ†
    chinese_tokens = split_with_synonyms(keyword)
    return material_tokens, digit_tokens, chinese_tokens


def expand_keyword_with_synonyms(keyword: str) -> list[str]:
    """
    Expands a keyword into a list of queries with synonyms replaced.
    This happens BEFORE tokenization.
    Example: "ç›´æ¥dn20" -> ["ç›´æ¥dn20", "ç›´æ¥å¤´dn20", "ç›´é€šdn20"]
    """
    # Create a map from a synonym to its group for easy lookup.
    synonym_to_group = {syn: group for group in SYNONYM_GROUPS for syn in group}
    # Sort all available synonyms by length, descending.
    # This ensures that longer synonyms (e.g., "å¼‚å¾„ç›´é€š") are matched before
    # their shorter substrings (e.g., "ç›´é€š").
    sorted_synonyms = sorted(synonym_to_group.keys(), key=len, reverse=True)

    # Use a set for the initial list of queries to handle duplicates.
    queries = {keyword}

    # Iterate through the sorted list of synonyms.
    for syn in sorted_synonyms:
        # Create a temporary list to hold newly generated queries.
        new_queries = set()
        for q in queries:
            # If the synonym is found in the current query...
            if syn in q:
                # ...get its synonym group.
                group = synonym_to_group[syn]
                # And for each synonym in that group...
                for replacement in group:
                    # ...create a new query by replacing the original synonym.
                    new_queries.add(q.replace(syn, replacement))
        
        # After checking all existing queries for a given synonym,
        # update the main set of queries with the new variations.
        # This prevents replacement loops and combinatorial explosion within a single pass.
        if new_queries:
            queries.update(new_queries)

    return list(queries)


def search_with_keywords(df, keyword, field, strict=True, return_score=False):
    # --- MODIFIED: Expand keyword with synonyms before processing ---
    expanded_keywords = expand_keyword_with_synonyms(keyword.strip())
    all_results = {} # Use dict to store unique results with the best score

    for kw in expanded_keywords:
        material_tokens, _, chinese_tokens = classify_tokens(kw)

        query_size_tokens = {t for t in chinese_tokens if re.search(r'\d', t) and not t.endswith('Â°')}
        query_text_tokens = {t for t in chinese_tokens if not (re.search(r'\d', t) and not t.endswith('Â°'))}

        # 1. ä¸ºæ¯ä¸ªæŸ¥è¯¢è§„æ ¼ï¼Œåˆ›å»ºä¸€ä¸ªåŒ…å«æ‰€æœ‰ç­‰ä»·å†™æ³•çš„é›†åˆ
        query_spec_equivalents = {}
        query_material = material_tokens[0] if material_tokens else None
        for token in query_size_tokens:
            query_spec_equivalents[token] = expand_token_with_synonyms_and_units(token, material=query_material)
        
        for row in df.itertuples(index=False):
            # Use a unique identifier for each row to handle duplicates
            row_identifier = getattr(row, "Describrition", str(row)) 
            raw_text = str(getattr(row, field, ""))
            normalized_text = normalize_material(raw_text)

            if not all(m in normalized_text for m in material_tokens):
                continue

            product_all_tokens = split_with_synonyms(raw_text)
            text_specs = {t for t in product_all_tokens if re.search(r'\d', t)}
            
            if len(query_size_tokens) > len(text_specs):
                continue
    
            if query_size_tokens:
                unmatched_text_specs = text_specs.copy()
                all_query_specs_matched = True
                for q_spec, q_equivalents in query_spec_equivalents.items():
                    match_found = False
                    for t_spec in list(unmatched_text_specs):
                        if t_spec in q_equivalents:
                            match_found = True
                            unmatched_text_specs.remove(t_spec)
                            break
                    if not match_found:
                        all_query_specs_matched = False
                        break
                
                if not all_query_specs_matched:
                    continue

            material_keywords_in_query = {'pvc', 'ppr'}.intersection(query_text_tokens)
            if material_keywords_in_query:
                if not any(mat in normalized_text.lower() for mat in material_keywords_in_query):
                    continue

            hit_count = len(query_size_tokens)
            
            if strict:
                if not all(t in normalized_text.lower() for t in query_text_tokens):
                    continue
                hit_count += len(query_text_tokens)
            else:
                product_text_lower = normalized_text.lower()
                for token in query_text_tokens:
                    if token in product_text_lower:
                        hit_count += 1
                if query_text_tokens and hit_count == len(query_size_tokens):
                    continue
            
            total_tokens = len(query_size_tokens) + len(query_text_tokens)
            score = hit_count / total_tokens if total_tokens > 0 else 1
            
            # Store or update the result if the score is higher
            if row_identifier not in all_results or score > all_results[row_identifier][1]:
                all_results[row_identifier] = (row, score)

    # Convert the results dict back to a list
    final_results = list(all_results.values())
    # Sort results by score, descending
    final_results.sort(key=lambda x: x[1], reverse=True)

    if return_score:
        return final_results
    else:
        return [res[0] for res in final_results]

# â€” Session State åˆå§‹åŒ– â€”
for k, default in [("cart",[]),("last_out",pd.DataFrame()),("to_cart",[]),("to_remove",[])]:
    if k not in st.session_state:
        st.session_state[k] = default
#æŠŠç”¨æˆ·é€‰ä¸­çš„æŸ¥è¯¢ç»“æœæ¡ç›®åŠ å…¥è´­ç‰©è½¦ï¼Œå¹¶æ¸…ç©ºæœ¬æ¬¡é€‰æ‹©ï¼Œæ”¯æŒå¤šé€‰æ‰¹é‡æ·»åŠ 
def add_to_cart():
    for i in st.session_state.to_cart:
        st.session_state.cart.append(st.session_state.last_out.loc[i].to_dict())
    # æ¸…ç©ºé€‰æ‹©ï¼ˆæ¨èç”¨ pop æˆ– delï¼‰
    if "to_cart" in st.session_state:
        del st.session_state["to_cart"]

#åˆ é™¤è´­ç‰©è½¦ä¸­çš„æ¡ç›®ï¼Œæ”¯æŒå¤šé€‰æ‰¹é‡åˆ é™¤
def remove_from_cart():
    idxs = set(st.session_state.to_remove)
    st.session_state.cart = [it for j,it in enumerate(st.session_state.cart) if j not in idxs]
    if "to_remove" in st.session_state:
        del st.session_state["to_remove"]

# â€” ä¾§è¾¹æ å¯¼èˆª â€”
st.sidebar.header("å¯¼  èˆª")
page = st.sidebar.radio("æ“ä½œ", ["æŸ¥è¯¢äº§å“", "æ‰¹é‡æŸ¥è¯¢", "æ·»åŠ äº§å“", "åˆ é™¤äº§å“"])
st.sidebar.markdown("---")
st.sidebar.caption("Powered by Streamlit")


# é¡µé¢åˆ‡æ¢å’Œä¸»é€»è¾‘
if page == "æŸ¥è¯¢äº§å“":
    st.header("äº§å“æŸ¥è¯¢ç³»ç»Ÿ")
    df = load_data()
#è¾“å…¥æ¡†å¸ƒå±€
    c1, c3 = st.columns([6,1])
    with c1:
        keyword = st.text_input(
            "å…³é”®è¯ï¼ˆåç§°ã€è§„æ ¼ã€æè´¨å¯ä¸€èµ·è¾“å…¥ï¼‰",
            key="keyword"
        )
    with c3:
        qty = st.number_input(
            "æ•°é‡", min_value=1, key="qty"
        )
    mat_kw = st.text_input(
        "ç‰©æ–™å·æœç´¢", key="mat_kw"
    )
    price_type = st.selectbox(
        "ä»·æ ¼å­—æ®µ", ["å‡ºå‚ä»·_å«ç¨","å‡ºå‚ä»·_ä¸å«ç¨"],
        key="price_type"
    )
    fuzzy_mode = st.checkbox(
        "æœªæŸ¥åˆ°ç»“æœæ—¶å¯ç”¨æ¨¡ç³ŠæŸ¥æ‰¾ï¼ˆå¹¶æ˜¾ç¤ºåŒ¹é…åº¦ï¼‰",
        key="fuzzy_mode"
    )
    debug_mode = st.checkbox("å¼€å¯è°ƒè¯•æ¨¡å¼ (æ˜¾ç¤ºå…³é”®è¯è§£æç»“æœ)", key="debug_mode")

    #æŸ¥è¯¢æŒ‰é’®
    query_c1, query_c2, _ = st.columns([2, 2, 8])

    with query_c1:
        if st.button("æŸ¥è¯¢", use_container_width=True):
            keyword = st.session_state.get("keyword", "").strip()

            # å¦‚æœå¼€å¯è°ƒè¯•æ¨¡å¼ï¼Œåˆ™æ˜¾ç¤ºè§£æç»“æœ
            if st.session_state.get("debug_mode", False) and keyword:
                with st.expander("ğŸ” è°ƒè¯•ä¿¡æ¯ï¼šå…³é”®è¯è§£æç»“æœ", expanded=True):
                    _, _, chinese_tokens = classify_tokens(keyword)
                    st.write("**åŸå§‹è¾“å…¥:**")
                    st.code(keyword, language=None)
                    st.write("**å½’ä¸€åŒ–å (ç”¨äºéƒ¨åˆ†åŒ¹é…):**")
                    st.code(normalize_material(keyword), language=None)
                    st.write("**æœ€ç»ˆè§£æå‡ºçš„ Tokens (ç”¨äºæœç´¢):**")
                    st.write(chinese_tokens)
                    st.info("æç¤ºï¼šæœç´¢æ—¶ä¼šç”¨ä¸Šé¢çš„ Tokens å»åŒ¹é…æ•°æ®åº“ä¸­çš„äº§å“æè¿°ã€‚è¯·æ£€æŸ¥ Tokens æ˜¯å¦ç¬¦åˆæ‚¨çš„é¢„æœŸã€‚")
                st.markdown("---")

            out_df = pd.DataFrame()
            qty = st.session_state.qty if "qty" in st.session_state else 1
            
            # æ ¹æ®ä»·æ ¼å­—æ®µé€‰æ‹©ï¼ŒåŠ¨æ€å†³å®šè¦æ˜¾ç¤ºçš„åˆ—
            base_cols = ["Material", "Describrition", "æ•°é‡"]
            price_col = st.session_state.price_type
            show_cols = base_cols + [price_col]

            # ä¼˜å…ˆç‰©æ–™å·ç²¾ç¡®æŸ¥æ‰¾
            mat_kw = st.session_state.get("mat_kw", "").strip()
            if mat_kw:
                filtered = df[df["Material"].astype(str).str.contains(mat_kw)]
                if not filtered.empty:
                    out_df = pd.DataFrame(filtered.copy())  # å¼ºåˆ¶DataFrame
                    out_df["æ•°é‡"] = qty
                    out_df = out_df[[col for col in show_cols if col in out_df.columns]]
                    st.session_state.last_out = out_df
                else:
                    st.session_state.last_out = pd.DataFrame()
                    st.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ç¬¦åˆæ¡ä»¶çš„äº§å“")
            else:
                # åŸæœ‰å…³é”®è¯æŸ¥æ‰¾é€»è¾‘
                results = search_with_keywords(df, st.session_state.keyword, "Describrition", strict=True)
                #æ¨¡ç³ŠæŸ¥è¯¢
                if not results and st.session_state.fuzzy_mode:
                    fuzzy_results = search_with_keywords(df, st.session_state.keyword, "Describrition", strict=False, return_score=True)
                    if fuzzy_results:
                        out_df = pd.DataFrame([r[0] for r in fuzzy_results])
                        out_df["åŒ¹é…åº¦"] = [round(r[1], 2) for r in fuzzy_results]
                        out_df = out_df.sort_values("åŒ¹é…åº¦", ascending=False)
                        out_df["æ•°é‡"] = qty
                        show_cols_fuzzy = show_cols + ["åŒ¹é…åº¦"]
                        out_df = out_df[[col for col in show_cols_fuzzy if col in out_df.columns]]

                        # -- ä¿®æ”¹ï¼šç›´æ¥è¿”å›æ‰€æœ‰æ¨¡ç³ŠæŸ¥è¯¢ç»“æœï¼Œè€Œä¸æ˜¯åªæ˜¾ç¤ºå‰ä¸‰ååŒ¹é…åº¦çš„ç»“æœ --
                        st.session_state.last_out = out_df
                    else:
                        st.session_state.last_out = pd.DataFrame()
                        st.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ç¬¦åˆæ¡ä»¶çš„äº§å“")
                #ç²¾å‡†æŸ¥è¯¢
                elif results:
                    out_df = pd.DataFrame(results)
                    out_df["æ•°é‡"] = qty
                    out_df = out_df[[col for col in show_cols if col in out_df.columns]]
                    st.session_state.last_out = out_df
                else:
                    st.session_state.last_out = pd.DataFrame()
                    st.warning("âš ï¸ æœªæŸ¥è¯¢åˆ°ç¬¦åˆæ¡ä»¶çš„äº§å“")

    with query_c2:
        # The AI button is only active if there are fuzzy results to choose from
        can_ai_select = (
            "last_out" in st.session_state and
            not st.session_state.last_out.empty and
            "åŒ¹é…åº¦" in st.session_state.last_out.columns
        )
        if st.button("ğŸ¤– AI ä¼˜é€‰", use_container_width=True, disabled=not can_ai_select):
            with st.spinner("ğŸ¤– AI æ­£åœ¨åˆ†ææœ€ä½³åŒ¹é…..."):
                top_3_df = st.session_state.last_out.head(3)
                best_choice_df, message = ai_select_best_with_gpt(
                    st.session_state.keyword, top_3_df
                )
            
            if best_choice_df is not None:
                # Add to cart
                item_to_add = best_choice_df.iloc[0].to_dict()
                st.session_state.cart.append(item_to_add)
                st.success("âœ… AIå·²ä¸ºæ‚¨é€‰æ‹©äº§å“å¹¶åŠ å…¥è´­ç‰©è½¦ï¼")
                st.rerun() # To refresh cart view
            else:
                st.error(message)


    # æŸ¥è¯¢ç»“æœå±•ç¤ºå’Œè´­ç‰©è½¦æ“ä½œï¼ˆæ— è®ºæ˜¯å¦åˆšç‚¹äº†æŸ¥è¯¢æŒ‰é’®ï¼Œåªè¦æœ‰ç»“æœéƒ½æ˜¾ç¤ºï¼‰
    out_df = st.session_state.get("last_out", pd.DataFrame())
    if not out_df.empty and isinstance(out_df, pd.DataFrame):
        st.dataframe(out_df, use_container_width=True)
        def format_row(i):
            try:
                row = out_df.loc[i]
                if "äº§å“æè¿°" in out_df.columns:
                    return row["äº§å“æè¿°"]
                elif "Describrition" in out_df.columns:
                    return row["Describrition"]
                elif "Material" in out_df.columns:
                    return str(row["Material"])
                else:
                    return str(i)
            except Exception:
                return str(i)
        to_cart = st.multiselect(
            "é€‰æ‹©è¦åŠ å…¥è´­ç‰©è½¦çš„è¡Œ",
            options=list(out_df.index),
            format_func=format_row,
            key="to_cart"
        )
        if st.button("æ·»åŠ åˆ°è´­ç‰©è½¦", key="add_cart"):
            for i in to_cart:
                st.session_state.cart.append(out_df.loc[i].to_dict())
            if "to_cart" in st.session_state:
                del st.session_state["to_cart"]
            st.success("âœ… å·²åŠ å…¥è´­ç‰©è½¦")

    # è´­ç‰©è½¦åªåœ¨æœ‰å†…å®¹æ—¶æ˜¾ç¤º
    if st.session_state.cart:
        cart_df = pd.DataFrame(st.session_state.cart)
        st.dataframe(cart_df, use_container_width=True)
        to_remove = st.multiselect(
            "é€‰æ‹©è¦åˆ é™¤çš„è´­ç‰©è½¦æ¡ç›®",
            options=list(cart_df.index),
            format_func=lambda i: cart_df.loc[i, "äº§å“æè¿°"] if "äº§å“æè¿°" in cart_df.columns else cart_df.loc[i, "Describrition"],
            key="to_remove"
        )
        if st.button("åˆ é™¤æ‰€é€‰", key="del_cart_bottom"):
            idxs = set(to_remove)
            st.session_state.cart = [it for j, it in enumerate(st.session_state.cart) if j not in idxs]
            if "to_remove" in st.session_state:
                del st.session_state["to_remove"]
            st.rerun()

elif page == "æ‰¹é‡æŸ¥è¯¢":
    st.header("ğŸ“¦ æ‰¹é‡å¯¼å…¥æŸ¥è¯¢")
    st.info("è¯·ä¸Šä¼ ä¸€ä¸ª Excel (.xlsx) æˆ– CSV (.csv) æ–‡ä»¶ã€‚æ–‡ä»¶ä¸­éœ€è¦åŒ…å« **åç§°**ã€**è§„æ ¼** å’Œ **æ•°é‡** åˆ—ã€‚")

    uploaded_file = st.file_uploader(
        "ä¸Šä¼ æŸ¥è¯¢æ–‡ä»¶",
        type=["xlsx", "csv"],
        key="batch_file_uploader"
    )

    if uploaded_file is not None:
        # ä¸ºäº†é¿å…åœ¨æ¯æ¬¡äº¤äº’æ—¶éƒ½é‡æ–°è¯»å–æ–‡ä»¶ï¼Œæˆ‘ä»¬å°†å…¶å­˜å‚¨åœ¨ä¼šè¯çŠ¶æ€ä¸­
        # å¹¶æ£€æŸ¥ä¸Šä¼ çš„æ–‡ä»¶æ˜¯å¦æ˜¯æ–°çš„
        if 'query_df' not in st.session_state or st.session_state.get('uploaded_filename') != uploaded_file.name:
            try:
                if uploaded_file.name.endswith('.csv'):
                    st.session_state.query_df = pd.read_csv(uploaded_file)
                else:
                    st.session_state.query_df = pd.read_excel(uploaded_file)
                st.session_state.uploaded_filename = uploaded_file.name
            except Exception as e:
                st.error(f"è¯»å–æ–‡ä»¶æ—¶å‡ºé”™: {e}")
                st.stop()

        query_df = st.session_state.query_df
        file_columns = query_df.columns.tolist()

        st.markdown("---")
        st.subheader("è¯·ä¸ºæŸ¥è¯¢æŒ‡å®šåˆ—")

        c1, c2, c3 = st.columns(3)
        with c1:
            name_col = st.selectbox("åç§°æ‰€åœ¨åˆ—", options=file_columns, key="batch_name_col")
        with c2:
            spec_col = st.selectbox("è§„æ ¼æ‰€åœ¨åˆ—", options=file_columns, key="batch_spec_col")
        with c3:
            quantity_col = st.selectbox("æ•°é‡æ‰€åœ¨åˆ—", options=file_columns, key="batch_quantity_col")


        if st.button("ğŸš€ å¼€å§‹æ‰¹é‡æŸ¥è¯¢", use_container_width=True):
            # æ•°æ®å¸§å·²åŠ è½½ï¼Œåˆ—åå·²é€‰æ‹©ã€‚æˆ‘ä»¬å¯ä»¥ç›´æ¥å¼€å§‹å¤„ç†ã€‚
            products_df = load_data()
            results_log = []
            
            progress_bar = st.progress(0, text="æ­£åœ¨å‡†å¤‡æ‰¹é‡æŸ¥è¯¢...")
            total_rows = len(query_df)
            
            with st.spinner("æ­£åœ¨é€æ¡æŸ¥è¯¢å¹¶ä½¿ç”¨ AI ä¼˜é€‰ï¼Œè¯·ç¨å€™..."):
                for index, row in query_df.iterrows():
                    progress_text = f"æ­£åœ¨å¤„ç†: {index + 1}/{total_rows}"
                    progress_bar.progress((index + 1) / total_rows, text=progress_text)
                    
                    # Combine name and spec, then clean it
                    name_val = str(row[name_col]) if pd.notna(row[name_col]) else ""
                    spec_val = str(row[spec_col]) if pd.notna(row[spec_col]) else ""
                    
                    # å…³é”®ä¿®æ­£ï¼šç›´æ¥åˆå¹¶ï¼Œä¸å†è¿›è¡Œç‹¬ç«‹çš„æ ‡ç‚¹æ¸…ç†ã€‚
                    # æ‰€æœ‰çš„æ¸…ç†å’Œè§£æéƒ½ç»Ÿä¸€ç”± search_with_keywords å‡½æ•°å¤„ç†ï¼Œä»¥ä¿è¯é€»è¾‘ä¸€è‡´ã€‚
                    keyword = f"{name_val} {spec_val}".strip()
                    
                    # Ensure quantity is a valid number, default to 1 if not
                    try:
                        quantity = int(row.get(quantity_col, 1))
                    except (ValueError, TypeError):
                        quantity = 1


                    best_choice_df = None
                    status = "æœªæ‰¾åˆ°"

                    # Step 1: Strict search
                    strict_results = search_with_keywords(products_df, keyword, "Describrition", strict=True)
                    
                    if strict_results:
                        candidates_df = pd.DataFrame(strict_results)
                        # Use AI to select from strict results (take top 5 to be safe with token limits)
                        best_choice_df, message = ai_select_best_with_gpt(keyword, candidates_df.head(5))
                        if message == "Success" and best_choice_df is not None and not best_choice_df.empty:
                            status = "âœ… AIä»ä¸¥æ ¼åŒ¹é…ç»“æœä¸­é€‰æ‹©"
                    
                    # Step 2: Fuzzy search if strict search gave no result for AI
                    if best_choice_df is None or best_choice_df.empty:
                        fuzzy_results = search_with_keywords(products_df, keyword, "Describrition", strict=False, return_score=True)
                        if fuzzy_results:
                            fuzzy_df = pd.DataFrame([r[0] for r in fuzzy_results])
                            fuzzy_df["åŒ¹é…åº¦"] = [r[1] for r in fuzzy_results]
                            fuzzy_df = fuzzy_df.sort_values("åŒ¹é…åº¦", ascending=False)
                            
                            # Use AI to select from top 3 fuzzy results
                            best_choice_df, message = ai_select_best_with_gpt(keyword, fuzzy_df.head(3))
                            if message == "Success" and best_choice_df is not None and not best_choice_df.empty:
                                status = "ğŸŸ¡ AIä»æ¨¡ç³ŠåŒ¹é…ç»“æœä¸­é€‰æ‹©"

                    # Step 3: Add to cart if AI made a selection
                    if best_choice_df is not None and not best_choice_df.empty:
                        selected_item = best_choice_df.iloc[0].to_dict()
                        selected_item['æ•°é‡'] = quantity
                        st.session_state.cart.append(selected_item)
                        results_log.append({
                            "æŸ¥è¯¢å…³é”®è¯": keyword,
                            "åŒ¹é…ç»“æœ": selected_item.get("Describrition", "N/A"),
                            "çŠ¶æ€": status
                        })
                    else:
                        results_log.append({
                            "æŸ¥è¯¢å…³é”®è¯": keyword,
                            "åŒ¹é…ç»“æœ": "---",
                            "çŠ¶æ€": "âŒ æœªæ‰¾åˆ°æˆ–AIæ— æ³•é€‰æ‹©"
                        })

            progress_bar.empty()
            st.success(f"ğŸ‰ æ‰¹é‡æŸ¥è¯¢å®Œæˆï¼")
            
            # Display results log
            st.subheader("æ‰¹é‡æŸ¥è¯¢ç»“æœæ—¥å¿—")
            if results_log:
                st.dataframe(pd.DataFrame(results_log), use_container_width=True)
            
            # Rerun to update the cart display on the main page if needed,
            # but showing it here might be better ux
            if st.session_state.cart:
                st.subheader("ğŸ›’ å½“å‰è´­ç‰©è½¦")
                st.dataframe(pd.DataFrame(st.session_state.cart), use_container_width=True)

elif page == "æ·»åŠ äº§å“":
    st.header(" æ·»åŠ æ–°äº§å“åˆ°æ•°æ®åº“")
    df0 = load_data()
    cols = df0.columns.tolist()

    with st.form("add_form"):
        new_vals = {}
        for col in cols:
            if col == "åºå·":
                continue
            label = col + ("ï¼ˆå¿…å¡«ï¼‰" if col in ["Describrition","å‡ºå‚ä»·_å«ç¨","å‡ºå‚ä»·_ä¸å«ç¨"] else "")
            dtype = df0[col].dtype
            if col in ["å‡ºå‚ä»·_å«ç¨","å‡ºå‚ä»·_ä¸å«ç¨"]:
                new_vals[col] = st.text_input(label, key=f"add_{col}")
            elif pd.api.types.is_integer_dtype(dtype):
                new_vals[col] = st.number_input(label, step=1, format="%d", key=f"add_{col}")
            elif pd.api.types.is_float_dtype(dtype):
                new_vals[col] = st.number_input(label, format="%.2f", key=f"add_{col}")
            else:
                new_vals[col] = st.text_input(label, key=f"add_{col}")

        submitted = st.form_submit_button("æäº¤æ–°å¢")

    if submitted:
        missing = [
            f for f in ["Describrition","å‡ºå‚ä»·_å«ç¨","å‡ºå‚ä»·_ä¸å«ç¨"]
            if not new_vals.get(f) or (isinstance(new_vals[f], str) and not new_vals[f].strip())
        ]
        if missing:
            st.error(f"âš ï¸ ä»¥ä¸‹å­—æ®µä¸ºå¿…å¡«ï¼š{', '.join(missing)}")
        else:
            insert_product(new_vals)
            load_data.clear()
            st.success("âœ… äº§å“å·²æ·»åŠ åˆ°æ•°æ®åº“ï¼")

else:
    st.header("ğŸ—‘ï¸ åˆ é™¤äº§å“")
    df = load_data()
    if df.empty:
        st.info("å½“å‰æ— äº§å“å¯åˆ é™¤ã€‚")
    else:
        materials = st.multiselect(
            "è¯·é€‰æ‹©è¦åˆ é™¤çš„äº§å“ (Material)",
            options=df["Material"].tolist(),
            format_func=lambda m: str(m)
        )
        if st.button("åˆ é™¤é€‰ä¸­äº§å“"):
            delete_products(materials)
            load_data.clear()
            st.success("âœ… åˆ é™¤æˆåŠŸï¼")




